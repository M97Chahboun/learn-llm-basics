# SimpleLLM - ุชุฏุฑูุจ ูููุฐุฌ ูุบูู ุชุนูููู  
**(Educational Language Model Training)**

ุชูููุฐู ูุจุณูุท ูุตุบูุฑ ุงูุญุฌู ุจูุบุฉ Python/NumPy ููุท ูููุงุฐุฌ ุงููุบุฉ **(Language Model)** ููุธูุฑ ููู ุชุชุนูู ุงูุดุจูุงุช ุงูุนุตุจูุฉ **(neural networks)** ุงูุชูุจุค ุจุงููุตูุต. ุชู ุจูุงุคู ูู ุงูุตูุฑ ุฏูู ุงุณุชุฎุฏุงู ุฃู ุฅุทุงุฑุงุช ุนูู **(deep learning frameworks)** ููุชุนูู ุงูุนูููุ ูุฐูู ูุชุญููู ุฃูุตู ูุถูุญ ุชุนูููู **(educational clarity)**.

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)  
[![NumPy](https://img.shields.io/badge/NumPy-Required-orange.svg)](https://numpy.org/)  
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ๐ฏ ูุง ุงูุฐู ุณุชุชุนูููุ  
**(What You'll Learn)**

ููุถูุญ ูุฐุง ุงููุดุฑูุน **ุงูููุงููู ุงูุฃุณุงุณูุฉ **(core concepts)** ูุชุฏุฑูุจ ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ **(Large Language Models)**:

- **Tokenization **(.tokenize()) ุชุญููู ุงููุตูุต ุฅูู ุชูุซููุงุช ุฑูููุฉ  
  - **Tokenization** ุนูู ูุณุชูู ุงููููุฉ (ููุงุณุจ ูููุจุชุฏุฆูู)  
  - **BPE **(Byte Pair Encoding) ุชูุณูู ุงููุตูุต ุฅูู ูุญุฏุงุช ูุฑุนูุฉ ุชูุณุชุฎุฏู ูู ููุงุฐุฌ **GPT** (ุนูู ูุณุชูู ุงูุฅูุชุงุฌ)

- **Embeddings**: ููู ุชุชุญููู ุงููููุงุช ุฅูู ูุชุฌูุงุช ูุซููุฉ **(dense vector representations)**  
- **Position-Aware Processing**: ููุงุฐุง ููู ุชุฑุชูุจ ุงููููุงุช ูู ุงููุบุฉุ  
- **Neural Network Forward Pass**: ููู ุชูููููุฏ ุงูุชูุจุคุงุชุ  
- **Backpropagation**: ููู ุชุชุนููู ุงูููุงุฐุฌ ูู ุฃุฎุทุงุฆูุงุ  
- **Gradient Descent**: ููู ุชูุญุณููู ุฃูุฒุงู ุงููููุฐุฌุ  
- **Next-Word Prediction**: ุงููููุฉ ุงูุฃุณุงุณูุฉ ูููุฐุฌุฉ ุงููุบุฉ  
- **Model Persistence**: ุญูุธ ุงููููุฐุฌ ุงููุฏุฑูุจ ูุชุญูููู ูุงุญููุง  

ูุซุงูู ููุทูุงุจุ ุงููุนูููููุ ุฃู ูุฃู ุดุฎุต ูุถููู ุญูู ููููุฉ ุนูู ููุงุฐุฌ ูุซู **ChatGPT** ูู ุงูุฏุงุฎู!

---

## ๐ ุงูุจุฏุก ุงูุณุฑูุน  
**(Quick Start)**

```bash
# ุงุณุชูุณุงุฎ ุงููุณุชูุฏุน
git clone https://github.com/m97chahboun/learn-llm-basics.git
cd learn-llm-basics

# ุชุซุจูุช ุงูุชุจุนูุงุช (NumPy ููุท!)
pip install numpy

# ุงูุฎูุงุฑ 1: Tokenizer ุนูู ูุณุชูู ุงููููุฉ (ุฃุณูู ููููู)
python simple/train_llm.py

# ุงูุฎูุงุฑ 2: Tokenizer BPE (ูุซู GPT-2ุ ููุงุณุจ ููุฅูุชุงุฌ)
python bpe/train_llm_bpe.py
```

---

## ๐ ูููู ุงููุดุฑูุน  
**(Project Structure)**

```
learn-llm-basics/
โโโ simple/                          # ุชูููุฐ ุจุณูุท ุนูู ูุณุชูู ุงููููุฉ
โ   โโโ train_llm.py                # ุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู Tokenizer ุนูู ูุณุชูู ุงููููุฉ
โ   โโโ load_and_use_model.py       # ุชุญููู ูุงุณุชุฎุฏุงู ุงููููุฐุฌ ุงููุฏุฑูุจ
โ   โโโ MODEL_USAGE_GUIDE.md        # ุฏููู ุงุณุชุฎุฏุงู ููุตูู
โ
โโโ bpe/                             # ุชูููุฐ BPE ุนูู ูุณุชูู ุงูุฅูุชุงุฌ
โ   โโโ train_llm_bpe.py            # ุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู Tokenizer BPE
โ   โโโ load_and_use_model_bpe.py   # ุชุญููู ูุงุณุชุฎุฏุงู ูููุฐุฌ BPE
โ   โโโ improved_tokenizer.py       # Tokenizer BPE ูุณุชูู
โ   โโโ demo_bpe_advantage.py       # ุนุฑุถ ุชูุงุนูู ููุฒุงูุง BPE
โ
โโโ models/                          # ุงูููุงุฐุฌ ุงููุญููุธุฉ (ุจุนุฏ ุงูุชุฏุฑูุจ)
โ   โโโ llm_model.pkl               # ุฃูุฒุงู ุงููููุฐุฌ ุงูุจุณูุท
โ   โโโ tokenizer.json              # ูุงููุณ Tokenizer ุงูุจุณูุท
โ   โโโ llm_model_bpe.pkl           # ุฃูุฒุงู ูููุฐุฌ BPE
โ   โโโ bpe_tokenizer/              # ูููุงุช Tokenizer BPE
โ       โโโ vocab.json              # ููุฑุฏุงุช BPE
โ       โโโ merges.json             # ุนูููุงุช ุงูุฏูุฌ ุงูููุชุนูููุฉ
โ       โโโ tokenizer_config.json   # ุฅุนุฏุงุฏุงุช Tokenizer
โ
โโโ README.md
```

---

## ๐ ูุณุงุฑุงู ููุชุนููู  
**(Two Learning Paths)**

### ุงููุณุงุฑ 1: ุนูู ูุณุชูู ุงููููุฉ (ููุตู ุจู ูููุจุชุฏุฆูู)  
**(Simple Word-Level)**

**ุงุจุฏุฃ ูู ููุง ุฅุฐุง ููุช ุฌุฏูุฏูุง ูู ูุนุงูุฌุฉ ุงููุบุฉ ุงูุทุจูุนูุฉ **(NLP):

```bash
# ุชุฏุฑูุจ ุงููููุฐุฌ
python simple/train_llm.py

# ุงุณุชุฎุฏุงู ุงููููุฐุฌ ุงููุฏุฑูุจ
python simple/load_and_use_model.py
```

**ุงููุฒุงูุง **(Pros)โ:
- ุณูู ุงูููู  
- ุฑุจุท ูุจุงุดุฑ ูุงุญุฏ ููุงุญุฏ ุจูู ุงููููุงุช  
- ุชุฏุฑูุจ ุณุฑูุน  
- ููุชุงุฒ ูุชุนููู ุงูุฃุณุงุณูุงุช  

**ุงูุนููุจ **(Cons)โ:
- ูุง ูุชุนุงูู ูุน ุงููููุงุช ุบูุฑ ุงููุฏุฑูุจุฉ (ูุญููููุง ุฅูู `<UNK>`)  
- ูุญุชุงุฌ ุฅูู ููุฑุฏุงุช ูุจูุฑุฉ ูู ุงูุชุทุจููุงุช ุงูุญููููุฉ  
- ูุง ููุณุชุฎุฏู ูู ุฃูุธูุฉ ุงูุฅูุชุงุฌ  

---

### ุงููุณุงุฑ 2: BPE ุนูู ูุณุชูู ุงูุฅูุชุงุฌ  
**(BPE Subword - Production-Grade)**

**ุงูุชูู ุฅูู ููุง ุจุนุฏ ููู ุงูุฃุณุงุณูุงุช**:

```bash
# ุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู Tokenizer BPE
python bpe/train_llm_bpe.py

# ุงุณุชุฎุฏุงู ูููุฐุฌ BPE
python bpe/load_and_use_model_bpe.py

# ุดุงูุฏ ูุฒุงูุง BPE ุชูุงุนูููุง
python bpe/demo_bpe_advantage.py
```

**ุงููุฒุงูุง **(Pros)โ:
- ูุชุนุงูู ูุน **ุฃู** ูููุฉ (ุญุชู ุบูุฑ ุงููุฏุฑูุจุฉ ูููุง!)  
- ููุฑุฏุงุช ูุนูุงูุฉ (300 ุฑูุฒ ูุบุทููู ููุงููู ุงูุชุฑููุจุงุช ุงูููููุฉ)  
- ููุณุชุฎุฏู ูู **GPT-2**ุ **GPT-3**ุ **GPT-4**ุ ู**BERT**  
- ููุชูุท ุงูุชุญูููุงุช ุงูุตุฑููุฉ (ูุซู: play / playing / played)  

**ุงูุนููุจ **(Cons)โ๏ธ:
- ุฃูุซุฑ ุชุนููุฏูุง ููููู  
- ุชุฏุฑูุจ ุฃุจุทุฃ ููููุงู  

---

## ๐ ููุงุฑูุฉ Tokenizers

### ูุซุงู: ุงูุชุนุงูู ูุน ูููุงุช ุบูุฑ ูุฏุฑูุจุฉ  
**(Handling Unseen Words)**

```python
# ุจูุงูุงุช ุงูุชุฏุฑูุจ ุงุญุชูุช ููุท: "cat"ุ "dog"ุ "play"

# Tokenizer ุจุณูุท ุนูู ูุณุชูู ุงููููุฉ
"cats"    โ <UNK>  โ (ููุฏุงู ุงููุนูู)
"playing" โ <UNK>  โ (ููุฏุงู ุงููุนูู)
"dogs"    โ <UNK>  โ (ููุฏุงู ุงููุนูู)

# Tokenizer BPE
"cats"    โ ["cat", "s"]           โ (ุงููุนูู ูุญููุธ)
"playing" โ ["play", "ing"]        โ (ุงููุนูู ูุญููุธ)
"dogs"    โ ["dog", "s"]           โ (ุงููุนูู ูุญููุธ)
"unbelievable" โ ["un", "believ", "able"]  โ (ูููุฉ ุฌุฏูุฏุฉ ุชูุงููุง!)
```

### ููุงุกุฉ ุงูููุฑุฏุงุช  
**(Vocabulary Efficiency)**

| ุงูููุน                     | ุญุฌู ุงูููุฑุฏุงุช       | ูุง ูููู ุชูุซููู                     |
|--------------------------|--------------------|----------------------------------|
| Simple Word-Level        | 30 ุฑูุฒูุง           | 30 ูููุฉ ููุท                      |
| BPE Subword              | 300 ุฑูุฒูุง          | ููุงููู ุงูุชุฑููุจุงุช ุงูููููุฉ         |

### ุงูุงุณุชุฎุฏุงู ูู ุงูุนุงูู ุงูุญูููู  
**(Real-World Usage)**

| ุงููููุฐุฌ                   | ููุน Tokenizer             | ุญุฌู ุงูููุฑุฏุงุช |
|--------------------------|---------------------------|--------------|
| **ูุฐุง ุงููุดุฑูุน (ุจุณูุท)**    | Word-level                | ~30          |
| **ูุฐุง ุงููุดุฑูุน (BPE)**     | BPE                       | 300          |
| GPT-2                    | BPE                       | 50,257       |
| GPT-3                    | BPE                       | 50,257       |
| BERT                     | WordPiece (ูุดุจู BPE)     | 30,522       |
| Claude                   | BPE-based                 | ~100,000     |

---

## ๐ป ุฃูุซูุฉ ุนูู ุงูุงุณุชุฎุฏุงู  
**(Usage Examples)**

### Tokenizer ุจุณูุท  
**(Simple Tokenizer)**

```python
from simple.train_llm import SimpleTokenizer

tokenizer = SimpleTokenizer()
tokenizer.fit(["the cat sat on the mat"])

# ุชุฑููุฒ
ids = tokenizer.encode("the cat sat")  # [5, 2, 4]

# ูู ุงูุชุฑููุฒ
text = tokenizer.decode([5, 2, 4])  # "the cat sat"
```

### Tokenizer BPE

```python
from bpe.improved_tokenizer import BPETokenizer

tokenizer = BPETokenizer(vocab_size=300)
tokenizer.train(["the cat sat on the mat", "cats like to play"])

# ุชุฑููุฒ - ูุชุนุงูู ูุน ูููุงุช ุบูุฑ ูุฏุฑูุจุฉ!
ids = tokenizer.encode("cats playing")  # [12, 15, 7, 23, 31, ...]

# ุนุฑุถ ุงูุชูุณูู ุงููุฑุนู
tokens = tokenizer.tokenize("playing")  # ["play", "ing"]
```

---

## ๐ ุงููุฎุฑุฌุงุช ุงููุชููุนุฉ  
**(Expected Output)**

### ุชุฏุฑูุจ Tokenizer ุงูุจุณูุท

```
POSITION-AWARE LANGUAGE MODEL TRAINING
======================================

Vocabulary size: 34 words
Training sentences: 20
Context window: 3 words

TRAINING
======================================
Epoch  15/150 | Loss: 2.8541 | LR: 0.100
Epoch  30/150 | Loss: 2.1234 | LR: 0.100
...

๐ ุงูุณูุงู: 'the cat sat'
   1. on         โโโโโโโโโโโโโโโโโโโโ 0.456
   2. the        โโโโโโโโ 0.189
```

### ุชุฏุฑูุจ Tokenizer BPE

```
TRAINING LLM WITH BPE TOKENIZER
======================================

STEP 1: TRAINING BPE TOKENIZER
Training BPE tokenizer...
โ BPE vocabulary: 287 tokens
โ Merges learned: 283

๐ ุฃูุซูุฉ ุนูู ุชุฌุฒุฆุฉ BPE:
   'cats' โ ['cat', 's', '</w>']
   'playing' โ ['play', 'ing', '</w>']
   'sleeping' โ ['sleep', 'ing', '</w>']

STEP 3: TRAINING
Epoch  15/150 | Loss: 2.7234 | LR: 0.100
...
```

---

## ๐๏ธ ุงูุจููุฉ ุงููุนูุงุฑูุฉ  
**(Architecture)**

### ุดุจูุฉ ุนุตุจูุฉ N-gram ุชุฑุงุนู ุงูููุถุน  
**(Position-Aware N-gram Neural Network)**

```
ุงููุฏุฎู: ["the", "cat", "sat"]
         โ       โ       โ
    Embedโ  Embedโ  Embedโ  (ุชูุซููุงุช ูุฑุชุจุทุฉ ุจุงูููุถุน - Position-specific embeddings)
         โ       โ       โ
         [ุฏูุฌ] โ [ูุชุฌู 96-ุจุนุฏูุง]
                โ
         ุทุจูุฉ ูุฎููุฉ (96 ูุญุฏุฉุ ุฏุงูุฉ tanh)
                โ
         ุทุจูุฉ ุฎุฑุฌ (ุจุญุฌู ุงูููุฑุฏุงุช)
                โ
            Softmax
                โ
         ุชูุฒูุน ุงุญุชูุงูู
```

### ููุงุฐุง ุงูุชูุซููุงุช ุงููุฑุชุจุทุฉ ุจุงูููุถุนุ  
**(Why Position-Specific Embeddings?)**

ุจุฎูุงู ุงููุชูุณุท ุงูุจุณูุทุ ุชุญุงูุธ ุงูุชูุซููุงุช ุงููุฑุชุจุทุฉ ุจุงูููุถุน ุนูู ุชุฑุชูุจ ุงููููุงุช:
- **ุงูููุถุน 1**: "the" ูุจุฏุงูุฉ ุฌููุฉ  
- **ุงูููุถุน 2**: "cat" ููุงุนู  
- **ุงูููุถุน 3**: "sat" ููุนู  

ููุง ููููู ุงููููุฐุฌ ูู ุงูุชูููุฒ ุจูู:
- "the cat sat" โ ูุชูุจุฃ ุจู "on"  
- "sat on the" โ ูุชูุจุฃ ุจูููุฉ ูุฎุชููุฉ  

---

## ๐ฏ ุงูููุฒุงุช ุงูุชูุงุนููุฉ  
**(Interactive Features)**

### ูุถุน ุงูุชูุงุนู ูุน ุงููููุฐุฌ ุงูุจุณูุท

```bash
python simple/load_and_use_model.py
```

```
> predict: the cat sat
๐ ุชูุจุคุงุช ุงููููุฉ ุงูุชุงููุฉ:
   1. on          (0.456)
   2. the         (0.189)

> generate: the cat
โจ ุชู ุชูููุฏู: 'the cat sat on the mat and dogs'

> quit
```

### ูุถุน ุงูุชูุงุนู ูุน ูููุฐุฌ BPE

```bash
python bpe/load_and_use_model_bpe.py
```

```
> tokenize: unbelievable
๐ค ุชูุณูู 'unbelievable':
   ุงูุฑููุฒ: ['un', 'believ', 'able', '</w>']
   ุงููุนุฑููุงุช: [45, 78, 23, 4]

> predict: cats are
๐ ุชูุจุคุงุช ุงูุฑูุฒ ุงูุชุงูู:
   1. sleep_      (0.412)
   2. play_       (0.234)

> generate: dogs like
โจ ุชู ุชูููุฏู: 'dogs like to play in the park with'
```

---

## ๐ ุนูููุฉ ุงูุชุฏุฑูุจ  
**(Training Process)**

### ุฌุฏูู ูุนุฏู ุงูุชุนููู  
**(Learning Rate Schedule)**
- **ุงูุญูุจุฉ 1-50**: LR = 0.10 (ุชุนููู ุณุฑูุน ูู ุงูุจุฏุงูุฉ)  
- **ุงูุญูุจุฉ 51-100**: LR = 0.05 (ุชุญุณูู)  
- **ุงูุญูุจุฉ 101-150**: LR = 0.02 (ุถุจุท ุฏููู)  

### ุชุทููุฑ ุฏุงูุฉ ุงูุฎุณุงุฑุฉ  
**(Loss Progression)**
- **ูู ุงูุจุฏุงูุฉ**: ~3.5 (ุชูุจุคุงุช ุนุดูุงุฆูุฉ)  
- **ุจุนุฏ 50 ุญูุจุฉ**: ~1.5 (ุจุฏุฃ ุชุนููู ุงูุฃููุงุท)  
- **ุจุนุฏ 150 ุญูุจุฉ**: ~0.8-1.2 (ุชูุจุคุงุช ุฌูุฏุฉ)  

---

## ๐ ุงููุฑููุงุช ุงูุฑุฆูุณูุฉ ูุน ููุงุฐุฌ LLM ุงูุญููููุฉ  
**(Key Differences from Production LLMs)**

| ุงูููุฒุฉ                  | SimpleLLM               | ููุงุฐุฌ ุงูุฅูุชุงุฌ (GPTุ Claude)         |
|------------------------|-------------------------|-------------------------------------|
| **ุงููุนููุงุช **(Parameters)| ~10,000                 | 1 ูููุงุฑ โ 175 ูููุงุฑ+                |
| **ุงูุจููุฉ **(Architecture)| N-gram ูุน ููุถุน         | Transformers ุฐุงุช Self-Attention ูุชุนุฏุฏุฉ ุงูุฑุคูุณ |
| **ูุงูุฐุฉ ุงูุณูุงู**        | 3 ูููุงุช/ุฑููุฒ            | 4,000 โ 200,000+ ุฑูุฒ               |
| **ุจูุงูุงุช ุงูุชุฏุฑูุจ**      | 20-30 ุฌููุฉ              | ุชุฑูููููุงุช ุงูุฑููุฒ                   |
| **ุงูุชุฌุฒุฆุฉ **(Tokenization)| Word-level ุฃู BPE (300)| BPE/WordPiece (30Kโ100K)           |
| **ุงูุชูุซููุงุช **(Embeddings)| ูุฑุชุจุทุฉ ุจุงูููุถุน         | Learned + Positional Encoding       |
| **ุงูุทุจูุงุช**             | ุทุจูุชุงู                  | 12โ96+ ุทุจูุฉ Transformer             |
| **ุงูุงูุชุจุงู **(Attention)| ูุง ููุฌุฏ                 | Multi-head Self-Attention           |
| **ููุช ุงูุชุฏุฑูุจ**         | ุซูุงูู                   | ุฃุณุงุจูุน/ุฃุดูุฑ ุนูู ุขูุงู ูุญุฏุงุช GPU      |

---

## ๐๏ธ ุงูุชุฎุตูุต  
**(Customization)**

### ุชุบููุฑ ุญุฌู ููุฑุฏุงุช Tokenizer

```python
# Tokenizer ุจุณูุท - ูููููุฏ ุชููุงุฆููุง ูู ุจูุงูุงุช ุงูุชุฏุฑูุจ

# Tokenizer BPE - ุญุฏุฏ ุญุฌู ุงูููุฑุฏุงุช
tokenizer = BPETokenizer(vocab_size=500)  # ุฑููุฒ ุฃูุซุฑ = ุชุบุทูุฉ ุฃูุถู
```

### ุชุนุฏูู ุจููุฉ ุงููููุฐุฌ

```python
model = NGramLLM(
    vocab_size=vocab_size,
    context_size=5,      # ุณูุงู ุฃุทูู
    embed_dim=64         # ุชูุซููุงุช ุฃูุจุฑ
)
```

### ุชุนุฏูู ุงูุชุฏุฑูุจ

```python
epochs = 200           # ุชุฏุฑูุจ ููุชุฑุฉ ุฃุทูู
lr = 0.15              # ูุนุฏู ุชุนููู ุฃุนูู
temperature = 0.7      # ุชูููุฏ ุฃูุซุฑ ุชุฑููุฒูุง
```

---

## ๐งช ุชุฌุงุฑุจ ููููู ุชุฌุฑุจุชูุง  
**(Experiments to Try)**

1. **ูุงุฑู ุจูู Tokenizers** โ ุฏุฑูุจ ูููููุง ููุงุฑู ุงูุชูุจุคุงุช  
2. **ุงุฎุชุจุฑ ูููุงุช ุบูุฑ ูุฏุฑูุจุฉ** โ ุดุงูุฏ ููู ูุชุนุงูู BPE ูุน "extraordinary"ุ "unbelievable"  
3. **ุฒูุฏ ุญุฌู ุงูููุฑุฏุงุช** โ ุฌุฑูุจ BPE ุจุญุฌู 500 ุฃู 1000  
4. **ุฃุถู ุจูุงูุงุช ุฃูุซุฑ** โ ุฃุถู 50+ ุฌููุฉ ูุดุงูุฏ ุงูุชุญุณูู  
5. **ูุณูุน ูุงูุฐุฉ ุงูุณูุงู** โ ูู 3 ุฅูู 5 ุฃู 7  
6. **ุบููุฑ ุฏุฑุฌุฉ ุงูุญุฑุงุฑุฉ **(Temperature)โ ุฌุฑูุจ 0.3 (ูุฑููุฒ) ููุงุจู 1.5 (ุฅุจุฏุงุนู)  

---

## ๐ ููุงุฑุฏ ุชุนููููุฉ  
**(Educational Resources)**

### ุฃูุฑุงู ุจุญุซูุฉ  
**(Papers)**
- [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) โ Bengio et al. (2003)  
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) โ ุขููุฉ ุงูุงูุชุจุงู **(Attention mechanism)**  
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) โ ุจููุฉ Transformer  
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) โ GPT-3  

### ุฏุฑูุณ ุชูุถูุญูุฉ  
**(Tutorials)**
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)  
- [The Illustrated Word2vec](http://jalammar.github.io/illustrated-word2vec/)  
- [BPE Tokenization Explained](https://huggingface.co/learn/nlp-course/chapter6/5)  
- [Neural Networks from Scratch](https://nnfs.io/)  

### ูุชุจ  
**(Books)**
- *Speech and Language Processing* by Jurafsky & Martin  
- *Deep Learning* by Goodfellow, Bengio, and Courville  

---

## ๐ค ุงููุณุงููุฉ  
**(Contributing)**

ูุฑุญูุจ ุจุงููุณุงููุงุช! ุฃููุงุฑ:
- [ ] ุฅุถุงูุฉ ุขููุฉ ุงูุงูุชุจุงู **(attention mechanism)**  
- [ ] ุชูููุฐ beam search ููุชูููุฏ  
- [ ] ุชุตูุฑ ุงูุชูุซููุงุช ุจุงุณุชุฎุฏุงู t-SNE  
- [ ] ุฅุถุงูุฉ ูููุงุณ perplexity  
- [ ] ุชูููุฐ ุชุฏุฑูุจ ุจุงูุฏููุนุงุช ุงูุตุบูุฑุฉ **(mini-batch training)**  
- [ ] ุฅุถุงูุฉ ุชูุธูู ุจุงูุฅุณูุงุท **(dropout regularization)**  
- [ ] ุฏุนู ูุบุงุช ุฃุฎุฑู  
- [ ] ูุงุฌูุฉ ููุจ ุจุงุณุชุฎุฏุงู Gradio/Streamlit  

---

## ๐ ูุดููุงุช ุดุงุฆุนุฉ  
**(Common Issues)**

### "ููุณ ุงูุชูุจุคุงุช ููู ุงูุณูุงูุงุช"  
**ุงูุญู**: ุชุฃูุฏ ูู ุงุณุชุฎุฏุงู ุชูุซููุงุช ูุฑุชุจุทุฉ ุจุงูููุถุน **(position-specific embeddings)**ุ ูููุณ ุงููุชูุณุท.

### "ModuleNotFoundError"  
**ุงูุญู**: ุชุฃูุฏ ุฃูู ูู ุงูุฏููู ุงูุตุญูุญ:
```bash
python simple/train_llm.py  # ูููุณ train_llm.py ููุท
```

### "Tokenizer BPE ุบูุฑ ููุฌูุฏ"  
**ุงูุญู**: ุฏุฑูุจ ูููุฐุฌ BPE ุฃูููุง:
```bash
python bpe/train_llm_bpe.py
```

### "ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ูุง ุชูุฎูุถ"  
**ุญููู**:
- ุฒูุฏ ุนุฏุฏ ุงูุญูุจ (200+)
- ุบููุฑ ูุนุฏู ุงูุชุนููู (ุฌุฑูุจ 0.15)
- ุฃุถู ุจูุงูุงุช ุชุฏุฑูุจ ุฃูุซุฑ ุชููุนูุง
- ุชุญููู ูู ุชุฏูู ุงูุชุฏุฑุฌุงุช **(gradient flow)**

### "ุชูุจุคุงุช ุถุนููุฉ"  
**ุญููู**:
- ุฏุฑูุจ ููุชุฑุฉ ุฃุทูู (150+ ุญูุจุฉ)
- ุงุณุชุฎุฏู Tokenizer BPE ุจุฏููุง ูู ุงูุจุณูุท
- ุฃุถู ุจูุงูุงุช ุชุฏุฑูุจ ุฃูุซุฑ ุชููุนูุง
- ุฒูุฏ ุญุฌู ุงูููุฑุฏุงุช
- ุฒูุฏ ุฃุจุนุงุฏ ุงูุชูุซููุงุช **(embedding dimensions)**

---

## ๐ ุงูุชุฑุฎูุต  
**(License)**

ุชุฑุฎูุต MIT โ ุงุณุชุฎุฏูู ุจุญุฑูุฉ ููุชุนูููุ ุงูุชุฏุฑูุณุ ุฃู ุงูุจุญุซ!

---

## โ๏ธ ููุงุญุธุงุช ูููุฉ  
**(Important Notes)**

- **ุงูุบุฑุถ ุชุนูููู**: ูุฐุง ุฃุฏุงุฉ ุชุนูููุ ูููุณ ูุธุงู ุฅูุชุงุฌ  
- **ูุฏุฑุฉ ูุญุฏูุฏุฉ**: ูุชูุจุฃ ููุท ุจุงููููุฉ/ุงูุฑูุฒ ุงูุชุงูู ูู ุชุฏุฑูุจ ูุญุฏูุฏ  
- **ูุง ููุฌุฏ ุงูุชุจุงู**: ูุง ูุญุชูู ุนูู ุขููุฉ ุงูุงูุชุจุงู ุงูุชู ุชุฌุนู ููุงุฐุฌ LLM ุงูุญุฏูุซุฉ ูููุฉ  
- **ูุทุงู ุตุบูุฑ**: ููุงุฐุฌ LLM ุงูุญููููุฉ ุชุญุชูู ูููุงุฑุงุช ุงููุนููุงุช ูุชุฏุฑุจ ุนูู ูุฌููุนุงุช ุจูุงูุงุช ุถุฎูุฉ  
- **BPE ุฃูุถู**: ููุชุทุจููุงุช ุงูุนูููุฉุ ุงุณุชุฎุฏู ุฏุงุฆููุง BPE ุจุฏููุง ูู ุงูุชุฌุฒุฆุฉ ุนูู ูุณุชูู ุงููููุฉ  

---

## ๐ ุฎุทุฉ ุชุนูู ููุชุฑุญุฉ  
**(Learning Path Recommendation)**

1. **ุงูุฃุณุจูุน 1**: ุงุจุฏุฃ ุจู `simple/train_llm.py`  
   - ุชุนููู ุงูุชุฌุฒุฆุฉ ุงูุฃุณุงุณูุฉ  
   - ุชุนุฑูู ุนูู ุงูุชูุซููุงุช ูุงูุดุจูุงุช ุงูุนุตุจูุฉ  
   - ุดุงูุฏ ููู ูุคุซุฑ ุงูููุถุน  

2. **ุงูุฃุณุจูุน 2**: ุงูุชูู ุฅูู `bpe/train_llm_bpe.py`  
   - ููู ุงูุชุฌุฒุฆุฉ ุงููุฑุนูุฉ  
   - ุดุงูุฏ ููู ูุชุนุงูู BPE ูุน ูููุงุช ุฌุฏูุฏุฉ  
   - ูุงุฑูู ุจุงูุชุฌุฒุฆุฉ ุงูุจุณูุทุฉ  

3. **ุงูุฃุณุจูุน 3**: ุฌุฑูุจ ูุบููุฑ  
   - ุบููุฑ ุงููุนููุงุช  
   - ุฃุถู ุจูุงูุงุชู ุงูุฎุงุตุฉ  
   - ุดุงูุฏ ูุง ุชุนูููู ุงููููุฐุฌ  

4. **ุงูุฃุณุจูุน 4**: ุบูุต ุนููู  
   - ุงูุฑุฃ ุงูุฃูุฑุงู ุงููุฐููุฑุฉ  
   - ุชุนููู ุขููุฉ ุงูุงูุชุจุงู  
   - ุฏุฑุณ ุจููุฉ Transformers  

---

## ๐ ุงูุดูุฑ ูุงูุชูุฏูุฑ  
**(Acknowledgments)**

ูุณุชูุญู ูู:
- ุฃุนูุงู ููุดูุง ุจูุฌูู **(Yoshua Bengio)** ูู ููุงุฐุฌ ุงููุบุฉ ุงูุนุตุจูุฉ  
- ุจููุฉ Transformer (ูุงุณูุงูู ูุขุฎุฑูู)  
- ุณูุณูุฉ GPT (OpenAI)  
- BERT (Google)  
- ุงููุจุงุฏุฑุงุช ุงูุชุนููููุฉ ุงูุชู ุชุฌุนู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู ูุชูุงูู ุงูุฌููุน  
- ูุฌุชูุน ุงูุชุนููู ุงูุขูู ููุชูุญ ุงููุตุฏุฑ  

---

## ๐ฌ ููุชูุงุตู  
**(Contact)**

ุฃุณุฆูุฉุ ููุงุญุธุงุชุ ุงูุชุญ ุชุฐูุฑุฉ ุฃู ุชูุงุตู!

- **GitHub**: [m97chahboun/learn-llm-basics](https://github.com/m97chahboun/learn-llm-basics)  
- **Issues**: ุจููุบ ุนู ุฃุฎุทุงุก ุฃู ุงุทูุจ ููุฒุงุช  

---

**โญ ุฅุฐุง ุณุงุนุฏู ูุฐุง ูู ููู ููุงุฐุฌ ุงููุบุฉุ ูู ูุถูู ุงุถุบุท ูุฌูุฉ ุนูู ุงููุณุชูุฏุน!**

ุตูุน ุจุญุจู ๐ค ูููุชุนููููู ูู ูู ููุงู

---

## ๐ ูุง ุงูุชุงููุ  
**(What's Next?)**

ุจุนุฏ ุฅุชูุงู ูุฐุง ุงููุดุฑูุนุ ุงุณุชูุดู:
- **Transformers**: ุขููุฉ ุงูุงูุชุจุงู ูุงูุจููุฉ ุงูุญุฏูุซุฉ  
- **Fine-tuning**: ุชูููู ุงูููุงุฐุฌ ุงููุฏุฑูุจุฉ ูุณุจููุง ููููุงุช ูุญุฏุฏุฉ  
- **RAG**: ุงูุชูููุฏ ุงููุนุฒูุฒ ุจุงุณุชุฑุฌุงุน ุงููุนูููุงุช  
- **Prompt Engineering**: ุงุณุชุฎุฑุฌ ุฃูุตู ุงุณุชูุงุฏุฉ ูู ููุงุฐุฌ LLM  
- **LangChain**: ุงุจูู ุชุทุจููุงุช LLM  
- **ููุงุฐุฌ LLM ุญููููุฉ**: ุฌุฑูุจ ููุงุฐุฌ Hugging Face
